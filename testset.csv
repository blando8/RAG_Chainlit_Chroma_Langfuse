user_input,reference_contexts,reference,synthesizer_name
What role does IEEE play in the publication and licensing of research related to generative AI model safety?,"['1\nA Systematic Review of Open Datasets Used in\nText-to-Image (T2I) Gen AI Model Safety\nTrupti Bavalatti1∗, Osama Ahmed 2∗, Dhaval Potdar 2∗, Rakeen Rouf 2∗, Faraz Jawed 2∗,\nManish Kumar Govind 3, and Siddharth Krishnan 3\nAuthor’s Version: This is the author’s version of\nthe paper.\n© 2025 IEEE. This work is licensed under the Creative Com-\nmons Attribution 4.0 International License (CC BY 4.0). For\nthe definitive version, see 10.1109/ACCESS.2025.3539933.\nDisclaimer: This research involves topics that may\ninclude disturbing results. Any explicit content has\nbeen redacted, and potentially disturbing results\nhave been presented in a neutral and anonymized\nmanner to minimize emotional distress to the read-\ners.\nAbstract—Novel research aimed at text-to-image (T2I)\ngenerative AI safety often relies on publicly available\ndatasets for training and evaluation, making the quality\nand composition of these datasets crucial. This paper\npresents a comprehensive review of the key datasets used']","IEEE is involved in the publication of research related to generative AI model safety, as evidenced by the systematic review paper on open datasets used in text-to-image generative AI model safety, which is published under the IEEE. The work is licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0), indicating IEEE's role in ensuring the research is accessible and properly attributed.",single_hop_specifc_query_synthesizer
"What T2I datasets are good for research, and how they help with safety and ethics?","['datasets for training and evaluation, making the quality\nand composition of these datasets crucial. This paper\npresents a comprehensive review of the key datasets used\nin the T2I research, detailing their collection methods,\ncompositions, semantic and syntactic diversity of prompts\nand the quality, coverage, and distribution of harm types\nin the datasets. By highlighting the strengths and limi-\ntations of the datasets, this study enables researchers to\nfind the most relevant datasets for a use case, critically\nassess the downstream impacts of their work given the\ndataset distribution, particularly regarding model safety\nand ethical considerations, and also identify the gaps in\ndataset coverage and quality that future research may\naddress.\nI. I NTRODUCTION\nA. Overview of T2I Models and their Safety Stack\nThe advancement of T2I generative AI models\nis disrupting traditional content creation methodolo-\ngies, enabling users to generate highly photorealis-']","The paper presents a comprehensive review of key datasets used in T2I research, detailing their collection methods, compositions, semantic and syntactic diversity of prompts, and the quality, coverage, and distribution of harm types in the datasets. This study enables researchers to find the most relevant datasets for a use case, critically assess the downstream impacts of their work given the dataset distribution, particularly regarding model safety and ethical considerations, and also identify the gaps in dataset coverage and quality that future research may address.",single_hop_specifc_query_synthesizer
What are the ethical concerns associated with T2I models in AI?,"['A. Overview of T2I Models and their Safety Stack\nThe advancement of T2I generative AI models\nis disrupting traditional content creation methodolo-\ngies, enabling users to generate highly photorealis-\ntic, imaginative, or personalized images from textual\n1Meta, USA (e-mail: truptib@meta.com) and corresponding author\n2Duke University, USA (e-mail: {osama.shawir, dhaval.potdar,\nrakeen.rouf, faraz.jawed}@duke.edu)\n3University of North Carolina at Charlotte, USA (e-mail: skrish-\nnan@uncc.edu)\n*These authors contributed equally.\ndescriptions [1]. Although their ability to generate\nvisually compelling outputs based on simple tex-\ntual prompts has caused their widespread adoption\nacross industries [2], the open-ended nature of these\nmodels poses challenges in ensuring that the gener-\nated outputs align with ethical, cultural, and societal\nnorms [3]. The models can be misused for gen-\nerating harmful or inappropriate content, including\nviolent, explicit, or biased imagery as the models']","The ethical concerns associated with T2I models in AI include the potential misuse of these models to generate harmful or inappropriate content, such as violent, explicit, or biased imagery. The open-ended nature of these models poses challenges in ensuring that the generated outputs align with ethical, cultural, and societal norms.",single_hop_specifc_query_synthesizer
What be the importance of 23 Feb 2025 in the context of labeled datasets and safety in T2I systems?,"['or multimodal classifiers that can detect harmful\nprompt-image pairs [5].\nB. Role of Labeled Datasets in Safety\nLabeled datasets, which consist of input text\nprompts and the corresponding generated image re-\nsponses and annotations, whether they violate safety\nguidelines, are essential for building the components\nof the safety stack [5]. These datasets play a dual\nrole: they are used to train/fine-tune the foundation\nmodel or train classifiers that detect and block harm-\nful content, and they also serve as benchmarks for\nevaluating the safety performance of these classifiers\nor models. By capturing a diverse range of potential\nmisuse cases, labeled datasets enable the creation of\nrobust safety systems capable of mitigating the risks\nassociated with T2I systems.\narXiv:2503.00020v1  [cs.CL]  23 Feb 2025']","The date 23 Feb 2025 is associated with the arXiv identifier for a paper discussing the role of labeled datasets in building safety components for T2I systems, highlighting their use in training models and serving as benchmarks for evaluating safety performance.",single_hop_specifc_query_synthesizer
Why is dataset analysis crucial for T2I models?,"['2\nC. Importance of Dataset Analysis in T2I Models\nNovel research on T2I safety is increasingly\nbased on publicly available datasets; therefore,\nstudying the composition of datasets that contain\nlabeled prompt-to-image pairs is crucial for several\nreasons. These datasets are annotated to indicate\nwhether the ¡prompt input, image output¿ combi-\nnations violate safety guidelines and serve as a core\nresource for training and evaluating the performance\nof these safety components. Understanding the com-\nposition ensures that the dataset adequately repre-\nsents a wide range of harmful categories, including\nedge cases, which may challenge a model’s safety\nmechanisms. It helps to identify biases or gaps\nin coverage, such as over-representation of certain\ntypes of violations while under-representing others,\nwhich could result in unbalanced or ineffective clas-\nsifiers. Analysis of the dataset composition will also\nreveal potential ambiguities or inconsistencies in']","Studying the composition of datasets that contain labeled prompt-to-image pairs is crucial because it ensures that the dataset adequately represents a wide range of harmful categories, including edge cases, which may challenge a model’s safety mechanisms. It helps to identify biases or gaps in coverage, such as over-representation of certain types of violations while under-representing others, which could result in unbalanced or ineffective classifiers.",single_hop_specifc_query_synthesizer
What LLMs datasets important for?,"['ethical, and safe T2I models.\nII. R ELATED WORK\nDatasets are critical to the performance and\nsafety of generative models. Several studies have\nbeen done on the datasets used for training and\nfinetuning LLMs. [10] categorized large amount\nof datasets into pre-training corpora, instruction\nfine-tuning datasets, preference datasets, evaluation\ndatasets and traditional NLP datasets and studied\ntheir challenges. More recently, a comprehensive\nstudy of preference datasets used in fine-tuning\nhas been done in [11] and a study of 16 pre-train\ndatasets and 16 fine-tune datasets from qualitative\nperspective has been done in [12]. There have also\nbeen systematic studies of LLM benchmarks itself,\nfor instance [13] surveys MLLM benchmarks, re-\nviewing 211 benchmarks that assess MLLMs across\nunderstanding, reasoning, generation, and applica-\ntion and a detailed analysis of dataset constructions\nacross diverse modalities has been provided and\na critical assessment of 23 state-of-the-art LLM']","Datasets are critical to the performance and safety of generative models, including LLMs. They are categorized into pre-training corpora, instruction fine-tuning datasets, preference datasets, evaluation datasets, and traditional NLP datasets, each with its own challenges.",single_hop_specifc_query_synthesizer
"What are the ethical concerns associated with T2I models, and how do they relate to dataset biases and diversity?","['tion and a detailed analysis of dataset constructions\nacross diverse modalities has been provided and\na critical assessment of 23 state-of-the-art LLM\nbenchmarks using a unified evaluation framework\nthrough the lenses of people, process, and technol-\nogy has been done in [14].\nFor T2I applications, the research while not as\nextensive as on LLMs, is surely catching up. For\nsafety, several studies have focused on analyzing\nthe datasets used in T2I models and examining\ntheir biases, coverage, and diversity. Societal bias\nhas shown to exist in datasets [15] [16] [17] and\na study of widely used datasets for text-to-image\nsynthesis, including Oxford-102, CUB-200-2011,\nand COCO has been done in [18], calling inadequate\ndiversity of real-world scenes. Similarly, a study of\nthe LAION-400M dataset contains problematic con-\ntent across misogyny, pornography, and malignant\nstereotypes [19].\nMost existing research is on analyzing certain\ndatasets for specific harm types as shown above.']","For T2I applications, ethical concerns primarily revolve around the biases, coverage, and diversity of the datasets used. Studies have shown that societal bias exists in these datasets, and there is inadequate diversity of real-world scenes. Additionally, the LAION-400M dataset has been found to contain problematic content, including misogyny, pornography, and malignant stereotypes. These issues highlight the importance of analyzing datasets for specific harm types to ensure ethical compliance in T2I models.",single_hop_specifc_query_synthesizer
Wht is LatentGuardCoPro?,"['tent across misogyny, pornography, and malignant\nstereotypes [19].\nMost existing research is on analyzing certain\ndatasets for specific harm types as shown above.\nThere is a new body of work that is being done to\naddress the gaps in the datasets. DataPerf introduced\nthe Adversarial Nibbler challenge [28], emphasizing\nthe importance of gathering prompts to identify\nharmful or biased outputs in T2I models. Other stud-\nies, such as LatentGuardCoPro [22], have developed\ndatasets specifically targeting unsafe input prompts,\nfocusing on areas such as violence and hate speech.\nBy doing a systematic review of existing open\ndatasets for T2I safety, we hope to encourage more\nsuch research to build comprehensive safety datasets']","LatentGuardCoPro is a study that has developed datasets specifically targeting unsafe input prompts, focusing on areas such as violence and hate speech.",single_hop_specifc_query_synthesizer
What role does the LatentGuardCoPro dataset play in improving the safety of T2I models?,"['3\ncovering broad classes of harm, that are composed\nof diverse set of topics and are that are in several\nlanguages. One such systematic study of safety\ndatasets has been done for evaluating and improving\nsafety of LLMs [20]. Our research aims to do the\nsame and fill the gap for T2I model safety.\nIII. O VERVIEW OF KEY DATASETS\nIn this study, we compiled the most prominent\nopen-source datasets in the T2I model safety\ndomain. Table 1 presents the number of prompts\ncontained in each data source, with the majority\ncoming from the ”SafeGenExplicit56k” and\n”LatentGuardCoPro” datasets. The following is a\nbrief description of each dataset.\nSafeGenExplicit56k: The dataset comprises\n56,000 textual prompts reflecting real-world\ninstances of sexual exposure. The CLIP model\n[25] is utilized along with BLIP2 [23] to generate\nmultiple candidate text captions for a given explicit\nimage. The optimal prompt is selected based on the\nhighest CLIP score, ensuring the best alignment']","The LatentGuardCoPro dataset is one of the most prominent open-source datasets compiled in the study to improve the safety of T2I models. It is part of the effort to evaluate and enhance the safety of these models, alongside the SafeGenExplicit56k dataset.",single_hop_specifc_query_synthesizer
How does the ART framework utilize the Meta Dataset to enhance safety in text-to-image generative models?,"['multiple candidate text captions for a given explicit\nimage. The optimal prompt is selected based on the\nhighest CLIP score, ensuring the best alignment\nbetween the image and its textual description. [30]\nLatenGuardCoPro: This dataset is a component\nof the Latent Guard framework [22], aimed at\nimproving safety in T2I models by identifying\nand blocking unsafe input prompts. The CoPro\ndataset includes prompts focused on harmful topics\nsuch as violence, hate, and sexual content. It was\nspecifically developed for training and assessing\nthe Latent Guard framework with data generated\nby selecting a range of unsafe concepts and using\nan LLM to create prompts associated with each\nconcept.\nART: The Meta Dataset (MD), a key component\nof the ART (Automatic Red-Teaming) [24]\nframework, comprises unique prompts paired with\ncorresponding unsafe images collected from open-\nsource prompt websites. This dataset is categorized\ninto seven types of harmful content. The MD serves']","The ART (Automatic Red-Teaming) framework uses the Meta Dataset, which comprises unique prompts paired with corresponding unsafe images collected from open-source prompt websites. This dataset is categorized into seven types of harmful content, serving as a key component in enhancing safety in text-to-image generative models.",single_hop_specifc_query_synthesizer
