user_input,reference_contexts,reference,synthesizer_name
What role does the Intelligent Informatics and Service Innovation Research Center play in the development of Thai-centric large language models?,"['OPEN THAI GPT 1.6 AND R1: T HAI -CENTRIC OPEN -SOURCE\nAND REASONING LARGE LANGUAGE MODELS\nTECHNICAL REPORT\nSumeth Yuenyong*1, Thodsaporn Chay-intr†2,3 , and Kobkrit Viriyayudhakorn‡2,3,4\n1Department of Computer Science, Faculty of Engineering, Mahidol University, Thailand\n2iApp Technology Co., Ltd., Thailand\n3Intelligent Informatics and Service Innovation Research Center, Thailand\n4Artificial Intelligence Entrepreneur Association of Thailand (AIEAT), Thailand\nABSTRACT\nWe present OpenThaiGPT 1.6 and R1 (OTG-1.6 and OTG-R1), Thai-centric Large Language Models\n(LLMs) developed through distinct methodologies to enhance generalization and reasoning capabil-\nities. OTG-1.6 employs Task Arithmetic model merging for broad generalization, while OTG-R1\nintegrates multi-stage training with the Less-Is-More Reasoning Hypothesis (LIMO) for advanced\nreasoning. Benchmark evaluations demonstrate superior performance across Thai language tasks,']","The Intelligent Informatics and Service Innovation Research Center is involved in the development of Thai-centric large language models, as indicated by its association with the authors of the technical report on OpenThaiGPT 1.6 and R1, which are Thai-centric large language models developed to enhance generalization and reasoning capabilities.",single_hop_specifc_query_synthesizer
How have recent advancements in LLMs improved their performance in natural language tasks?,"['integrates multi-stage training with the Less-Is-More Reasoning Hypothesis (LIMO) for advanced\nreasoning. Benchmark evaluations demonstrate superior performance across Thai language tasks,\nachieving competitive results against larger-scale open-source Thai LLMs. This paper details the pro-\nposed models, training processes, benchmarks, and results, highlighting improvements over previous\nmodels and establishing new performance standards for Thai-centric LLMs.\n1 Introduction\nThe development of Large Language Models (LLMs) has significantly advanced natural language understanding,\nreasoning, and generation capabilities [OpenAI, 2024, Gemma, 2025, DeepSeek-AI, 2025, Xu et al., 2025]. Recent\nmodels have demonstrated impressive performance across various tasks by leveraging sophisticated architectures,\nextensive training data, and improved training methodologies [Qin et al., 2024, Luo et al., 2025, OpenAI, 2025].']","Recent advancements in Large Language Models (LLMs) have significantly enhanced their natural language understanding, reasoning, and generation capabilities. These improvements are attributed to sophisticated architectures, extensive training data, and improved training methodologies, leading to impressive performance across various tasks.",single_hop_specifc_query_synthesizer
What contribution did Goddard et al. make to the development of OpenThaiGPT 1.6?,"['extensive training data, and improved training methodologies [Qin et al., 2024, Luo et al., 2025, OpenAI, 2025].\nHowever, achieving optimal performance for Thai-centric LLMs remains challenging due to linguistic complexities,\nlimited high-quality datasets, and inadequate adaptation of general LLM architectures to Thai-specific tasks.\nAddressing these challenges requires techniques that enhance generalization, reasoning, and efficiency without ex-\ncessively increasing model scale. We present OpenThaiGPT 1.6 (OTG-1.6) and OpenThaiGPT R1 (OTG-R1),\ndeveloped using complementary methodologies to overcome these limitations. OTG-1.6 applies Task Arithmetic model\nmerging to combine specialized models, improving generalization across diverse tasks without increasing computational\nrequirements [Ilharco et al., 2023, Goddard et al., 2024]. Meanwhile, OTG-R1, inspired by DeepSeek-R1 [DeepSeek-']","Goddard et al. contributed to the development of OpenThaiGPT 1.6 by applying Task Arithmetic model merging to combine specialized models, which improves generalization across diverse tasks without increasing computational requirements.",single_hop_specifc_query_synthesizer
